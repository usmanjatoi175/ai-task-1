# task9_sentiment.py
# Simple Sentiment Analysis (Positive / Negative / Neutral)

import pandas as pd
import string
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import seaborn as sns

# ----------------------------
# 1. Create small dataset (manually)
# ----------------------------
data = {
    "text": [
        # Positive
        "I love this product", "The movie was amazing", "Great service and friendly staff",
        "This phone works perfectly", "I am very happy with the results", "The meal was delicious",
        "What a fantastic experience", "I enjoy using this app", "The teacher was supportive",
        "My vacation was wonderful",

        # Negative
        "I hate this service", "The food was terrible", "This product is a waste of money",
        "The experience was awful", "I am very disappointed", "The phone stopped working",
        "Worst customer service ever", "I regret buying this", "It was a horrible trip",
        "Nothing worked properly",

        # Neutral
        "I have a meeting tomorrow", "The weather is average today", "It is a normal day",
        "I will go to the store later", "The book is on the table", "She is reading a novel",
        "The device is black in color", "He lives in New York", "We are going to school",
        "My friend is watching TV"
    ],
    "label": [
        "positive","positive","positive","positive","positive","positive","positive","positive","positive","positive",
        "negative","negative","negative","negative","negative","negative","negative","negative","negative","negative",
        "neutral","neutral","neutral","neutral","neutral","neutral","neutral","neutral","neutral","neutral"
    ]
}

df = pd.DataFrame(data)
print("Dataset sample:")
print(df.head())

# ----------------------------
# 2. Preprocessing & Features
# ----------------------------
# Convert to lowercase, remove punctuation
def clean_text(text):
    text = text.lower()
    text = text.translate(str.maketrans('', '', string.punctuation))
    return text

df["clean_text"] = df["text"].apply(clean_text)

# Features using TF-IDF
vectorizer = TfidfVectorizer(stop_words="english")
X = vectorizer.fit_transform(df["clean_text"])
y = df["label"]

# ----------------------------
# 3. Train/Test Split
# ----------------------------
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# ----------------------------
# 4. Train Model (Naive Bayes)
# ----------------------------
model = MultinomialNB()
model.fit(X_train, y_train)

# ----------------------------
# 5. Evaluate
# ----------------------------
y_pred = model.predict(X_test)
print("\nAccuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred, labels=["positive","negative","neutral"])
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=["positive","negative","neutral"],
            yticklabels=["positive","negative","neutral"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

# ----------------------------
# 6. Test on New Sentences
# ----------------------------
new_sentences = [
    "I really enjoyed the concert",
    "This was the worst experience of my life",
    "The meeting is scheduled for tomorrow",
    "The food tastes great",
    "The phone battery dies quickly"
]

new_clean = [clean_text(s) for s in new_sentences]
X_new = vectorizer.transform(new_clean)
preds = model.predict(X_new)

print("\nNew Predictions:")
for sent, pred in zip(new_sentences, preds):
    print(f"{sent} --> {pred}")